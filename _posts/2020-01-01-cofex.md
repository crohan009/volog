---
layout: post
comments: false
title: "CoFEx - Code Feature Extractor"
date: 2020-03-01 00:01:01
tags: 
---

> CoFEx is a probabilistic deep neural network model designed to extract features from code (any programming lauguage) by combining laugage modelling (treating code as a natural language) and graph neural networks (treating code as a hierarchical graph), thereby bi-modally modelling code in order to create embeddings for programming languages for numerous applications such as code auto-completion, code prediction, bug detection and correction. 

<!--more-->

---
<h3> Contents </h3>

{: class="table-of-content"}
* TOC
{:toc}

---

## Intro


According to the **Naturalness Hypothesis** [[1](https://arxiv.org/abs/1709.06182)], software is a form of human communication, and just like natural languages, possesses probabilistic and statistical properties that can be exploited by machine learning (ML).  This has motivated the bimodal modelling of code that allows code to beinterpreted by humans and computers. Therefore, theoretically as our models of natural language improve, so will our ability to engineer and automate all aspects of software analysis and code generation. 

<p>
The trajectory of <b>ML on big code</b> has therefore followed the numerous break-throughs in the domain of Natural Language Understanding.  Token level models were the first to use the N-gram language models [2] to model code token statistics.  This was followed by sequence-models that utilized deep recurrent neural network models (RNNs and LSTMs) to better encode long and short term dependencies between tokens. Finally, neural attention models [3,4] have been the latest successors which work by paying "attention" to the local context of code to obtain token embeddings. </p>

<p>
The research in the field of Natural Language has been standardised with the <b>GLUE benchmark</b> [5] for comparing the numerous state of the art models. Consequently, there has been a progressive improvement of probabilistic language modelling with its applications trickling into everyday technologies such a word predictive keyboards and gmail’s sentence level email predictions. </p>

<p>
This post briefly illsutrates the NLP history and progresses towards motivating the CoFEx architecture, which draws inspirations from the current state-of-the-art NLP techniques.
</p>

## Context embeddings (A very Brief History)

- In language processing, **embeddings** are numbers or vectors of numbers that are assigned to tokens (eg: a unique vector assigned to every word in the english dictionary).
- Consequently, even a sentence, a paragraph, or an entire document may be represented by a vector of numbers.
- **Context embeddings** encode the context within which a particular token lies (eg: a word's embedding vector and it's contextual paragraph's embedding vector)

![word2vec_of_English]({{ '/assets/images/word2vec_embedding_projection.png' | relative_url }})
{: style="width: 100%;" class="center"}
*Fig. 1. A word2vec embedding of the english language  projected onto 2 dimensions. Notice how words with contextual similarities cluster together. <br> Source: [Louis's blog](https://devitrylouis.github.io/posts/2019/01/embeddings/)*

In general, there are two methods employed for extracting context embeddings ...

### 1. Autoregressive (AR) methods

Given a text sequence $$ x = [x_1,..., x_T]$$. (Assume this is a sentence with $$T$$ words in it.)<br>
AR models maximize the following likelihood objective function: <br>

$$ 
\underset{\theta}{\text{max}} \ \ log \ p_{\theta}(x) = \sum_{t=1}^T log p_{\theta}(x_t | x_{<t})
= \sum_{t=1}^T log \frac{exp \ ( h_{\theta}{(x_{1:t-1})}^T e(x_t))}{ \sum_{x'} exp \ ( h_{\theta}{(x_{1:t-1})}^T e(x')) }
$$ 

![AR_objective]({{ '/assets/images/AR_objective.png' | relative_url }})
{: style="width: 100%;" class="center"}

where, 
 - $$x_t$$ is the $$t^{th}$$ token in a sentence, and $$x_{<t}$$ is the sequence of tokens that come before $$x_t$$.
 - $$h_{\theta}{(x_{1:t-1})}^T$$ is the **context representation** (created by any probabilistic model, say a deep neural network architecture, which is parameterized by $$\theta$$)
 - $$e(x_t)$$ is the **embedding of x**, and 
 - $$exp \ ( h_{\theta}{(x_{1:t-1})}^T e(x_t))$$ is the **dot-product similarity** between a word embedding and its context representaiton. 

The context representation may be producted by any learning model (eg: language modelling, RNN, LSTM, Attention models like BERT, GPT-2, etc).

<h4> Limitations </h4>

1. Dependencies between the words on the left and the words on the right is missing
2. Even when combining forward and backward LMs (with bi-directional RNNs)


<h4><i> Examples </i></h4>

#### 1.1. [Word2Vec](https://arxiv.org/abs/1301.3781)

Maximizing the aforementioned Autogressive objective function on any dictinary of tokens gives us a **static embedding** of each of the tokens in the *n-dimensional* embedding space. This was a major break through in Natural Language processing back in 2013.

The figure below does this for tokens in the C++ programming language.

<br>

![word2vec of C/C++ tokens ]({{ '/assets/images/C_word2vec_embedding.png' | relative_url }})
{: style="width: 90%;" class="center"}
*Fig. 2. A word2vec embedding of tokens from C/C++ source code. <br> Source: [Harer et al - Automated software vulnerability detection with machine learning](https://arxiv.org/pdf/1803.04497.pdf)*

---

#### 1.2. RNN and its Variants (Sequence to Sequence models)

A natural progression was the usage of *neural networks* for generating these embeddings. <br>
Being a ideal universal function approximator, Neural networks (like RNNs) can take in a sequence of tokens and create embeddings that encode contextual information between words by looking at their temporal closeness.

![seq2seq models]({{ '/assets/images/seq_2_seq.png' | relative_url }})
{: style="width: 90%;" class="center"}
*Fig. 3. Seq2Seq:  encoder-decoder models that maps input sequences to output of sequences.*

---

#### 1.3. ELMo [(Embeddings from Language Models)](https://arxiv.org/abs/1802.05365)

ELMo extended RNN embeddings by looking at sequences in both directions (beginning to end and in reverse). This gives enbeddings bidirectional contexts as to where there are located. 

The following figure illustrates this procedure:
1. an [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) is used to look at each token ($$t_k$$).
2. ELMo takes bi-directional LSTMs to generate embeddings, concatentes the embeddings, and passes them through a neural network to generates a final bi-directional embedding.

![ELMo ]({{ '/assets/images/ELMo.png' | relative_url }})
{: style="width: 150%;" class="center"}
*Fig. 4. ELMo's Context embedding extraction mechanism.*

---


### 2. Autoencoding (AE) methods

Given a text sequence: $$ x = [x_1,..., x_T]$$ <br>
A corrupted version of x by randomly masking 15% of tokens: $$\hat{x}$$ *(this is <u>exactly</u> what [BERT](https://arxiv.org/abs/1810.04805) does)* <br>
Let the masked tokens be: $$\bar{x}$$ <br>
AE methods maximize the following objective:

$$
\underset{\theta}{\text{max}} \ \ log \ p_{\theta}( \bar{x} | \hat{x}) \approx \sum_{t=1}^T m_t log \ p_{\theta} (x_t | \hat{x}) = \sum_{t=1}^T m_t log \frac{ exp( H_{\theta} (\hat{x}))_{t}^T e(x_t) }{ \sum_{x'} exp( H_{\theta} (\hat{x}))_{t}^T ) e(x')}
$$

![AE_objective]({{ '/assets/images/AE_objective.png' | relative_url }})
{: style="width: 100%;" class="center"}

where, 
 - $$m_t$$: Indicates if $$x_t$$ is masked or not (0 or 1)
 - $$H_{\theta}(.)$$ : **Transformer**

<h4><i> Examples </i></h4>

#### 2.1. Attention Transformers

The core idea behind attention models is the *dot-product similarity*. The transformer architecture takes embeddinds and projects them into intermediate vectors (queries (Q) , keys (K), and values (V) ), and check the dot-product distance between ever query (Q) vector and it's context, i.e,the keys (K) of the context words (illustrated by the red and blue arrows below).

![Attention Transformers]({{ '/assets/images/att_transf.png' | relative_url }})
{: style="width: 150%;" class="center"}
*Fig. 5. Attention transformers. Source: [Vaswani et al - Attention is all you need.](https://arxiv.org/abs/1706.03762)*

For a fantastic explaination of attention models check out Jay's blog post [here](http://jalammar.github.io/illustrated-bert/) and [here](http://jalammar.github.io/illustrated-gpt2/).

#### 2.2. BERT

BERT brings together concepts form multiple papers:

![BERT's ideas]({{ '/assets/images/BERT_ideas.png' | relative_url }})
{: style="width: 60%;" class="center"}
*Fig. 6. BERT's ideas.*

- **ELM0** : bi-directional context embeddings
- **ULMFit** : Pretraining / Finetuning for NLP
- **Transformers**: Enocoder - Decoder transformers
- OpenAI **GPT**: Decoder stacked transformers 

<h4> BERT Pre-training </h4>

"Pre-training" is a **transfer learning** technique used to train a neural network model on vast amounts of data before fine-tuning it's particular problem. BERT is pre-trained by training it on two tasks:
1. **Masked Language Modelling**: basically, fill in the blanks.
2. **Two Sentence Task**:  given to sentences, are they related/in order? (Yes/No)

![BERT pretraining]({{ '/assets/images/BERT_pretraining.png' | relative_url }})
{: style="width: 150%;" class="center"}
*Fig. 7. BERT pre-trianing tasks: (1) Masked Language Modelling, and (2) Two Sentence Task.*

After pretraining BERT with a LOT of data, it may be fine-tuned for any specific *"down-stream"* task.

<h4> BERT's Fine-tuning </h4>

![BERT finetuning]({{ '/assets/images/BERT_finetuning.png' | relative_url }})
{: style="width: 150%;" class="center"}
*Fig. 8. BERT fine-trianing tasks (GLUE Benchmarks).*

--- 

<h4> Limitations of BERT </h4>

1. When predicting a masked token, other masked tokens are in the input - What if masked tokens are dependent?
2. Consecutive segment encodings are detached.
3. BERT uses Absolute positional encodings - consecutive segments do not have any relative positional information

#### 2.3. Graph Attention Methods: [Gated Graph Attention Networks](https://arxiv.org/abs/1803.07294)

So far we've looked at sentences which may be modelled as linear sequences. For information with an underlying graph structure (e.g.: the Abstract Syntax Tree (AST) of code), the context of a particular node is it's neighbours. In this context, we can utilize **Graph Neural Networks** (GNNs) for the job of generating context embeddings! <br>

Here's a pretty good survey paper of GNNs: (A Comprehensive Survey on Graph Neural Networks)[https://arxiv.org/abs/1901.00596]

The following figure illustrates a Graph Attention Network, a direct extension of the [Attention is all you Need](https://arxiv.org/abs/1706.03762) paper on graphs. 

![GaAN]({{ '/assets/images/GaAN.png' | relative_url }})
{: style="width: 150%;" class="center"}
*Fig. 9. Gated Graph Attention Networks.*


## CoFEx

To best model bimodal code properties, we look at code as (1) a sequence of natural language tokens which have long and short term bi-directional dependencies and (2) a programming language wherein there exist a structural hierarchy amongst the internal-types of the language. The latter can be derived from the **Abstract Syntax Tree AST)**  of any piece of code. Therefore, CoFEx extracts code features by two mechanisms: **language modelling** of code and **graph modelling** of the code AST tree.

The language model's architecture is based on the permutation language model utilized by [XLNet](https://arxiv.org/abs/1906.08237). This model performs an auto-regressive training on all possible permutations of code sequences by using stack of encoder blocks that perform the two-stream relative self-attention. We introduce graph modelling into the same framework by augmenting each encoder block to have two sub-blocks, the Language Model and the Graph Model. The Graph Model utilizes the Gated Graph Attention network ([GaAN](https://arxiv.org/abs/1803.07294)) to allow our model to selectively pay attention to the graph neighbourhood of tokens within the code AST tree. This equips our model to simultaneously embed each code token with its local context as well as its role in the code AST tree hierarchy.

### CoFEx architecture

CoFEx looks at a piece of code's AST and also treats it as a sequence. The following figure illustrates this mechanism.

![CoFEx architecture]({{ '/assets/images/cofex_arch_overview.png' | relative_url }})
{: style="width: 150%;" class="center"}
*Fig. 10. Illustration of the CoFEx architecture performing code Language Modelling (LM) with features extracted from code sequences and its AST.*


<i>**(to be continued ...)**</i>

## References

[1] Miltiadis Allamanis, Earl T Barr,  Premkumar Devanbu,  and Charles Sutton. A survey ofmachine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):81,2018.

[2] Veselin Raychev, Martin Vechev, and Eran Yahav. Code completion with statistical languagemodels. InAcm Sigplan Notices, volume 49, pages 419–428. ACM, 2014.

[3] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.  code2vec:  Learning distributedrepresentations of code.Proceedings of the ACM on Programming Languages, 3(POPL):40,2019.

[4] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Summarizing sourcecode  using  a  neural  attention  model.   InProceedings of the 54th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers), pages 2073–2083, 2016

[5] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.Glue: A multi-task benchmark and analysis platform for natural language understanding.arXivpreprint arXiv:1804.07461, 2018

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>

*Please don't hesitate to contact me with any errors. I'll be sure to correct them right away !*